{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Spam / Ham Message Classifier (Enhanced)\n",
    "\n",
    "Welcome to your custom spam classifier notebook! The goal of this notebook is to build, train, and export a machine learning model that can distinguish between 'spam' (unwanted messages) and 'ham' (legitimate messages).\n",
    "\n",
    "This enhanced version includes:\n",
    "- **Advanced Preprocessing**: Stopword removal and stemming.\n",
    "- **Data Analysis**: Justification for padding length.\n",
    "- **Robust Evaluation**: A full classification report and confusion matrix.\n",
    "- **Live Demo**: A cell to test predictions on new messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ 1. Install Dependencies\n",
    "\n",
    "First, we need to install the necessary Python libraries. \n",
    "- `tensorflow`: The core machine learning library for building and training our model.\n",
    "- `tensorflowjs`: A library to convert our trained TensorFlow model into a format that can run directly in a web browser.\n",
    "- `nltk`: A library for natural language processing, used here for stopword removal and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflowjs nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÅ 2. Load and Prepare the Dataset\n",
    "\n",
    "Next, we'll load our dataset. We're using the 'SMSSpamCollection' dataset, which is a collection of SMS messages already labeled as either spam or ham.\n",
    "\n",
    "**Action Required:** You'll need to upload the `SMSSpamCollection` file to your Colab environment. You can do this by clicking the **folder icon** on the left sidebar and then clicking the **upload button**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The dataset is a tab-separated file (.tsv), so we use sep='\\t'.\n",
    "# We also assign column names 'label' and 'message' for clarity.\n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "# Display the first 5 rows to verify it loaded correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßπ 3. Enhanced Text Preprocessing\n",
    "\n",
    "To improve accuracy, we'll perform more advanced text cleaning:\n",
    "- **Tokenization**: Splitting sentences into words.\n",
    "- **Stopword Removal**: Removing common words (like 'the', 'a', 'is') that don't add much meaning.\n",
    "- **Stemming**: Reducing words to their root form (e.g., 'running' becomes 'run')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text) # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with a single space\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Tokenization, Stopword Removal, and Stemming\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "df['cleaned'] = df['message'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä 4. Analyze Message Length Distribution\n",
    "\n",
    "To choose an optimal padding length (`maxlen`), it's helpful to understand the distribution of message lengths in our dataset. A fixed input length is required for our neural network, so we'll pad or truncate messages to fit. This plot helps us pick a value that covers most message lengths without adding excessive padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "message_lengths = df['cleaned'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(message_lengths, bins=50, kde=True)\n",
    "plt.title('Distribution of Message Lengths')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"From the chart, we can see that the vast majority of messages have fewer than 50 words. Choosing maxlen=50 is a reasonable choice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¢ 5. Tokenize and Pad Sequences\n",
    "\n",
    "Now we convert our preprocessed text into numerical sequences and ensure they all have the same length (50), as determined from our analysis above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Use the top 5000 most frequent words for our vocabulary\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df['cleaned'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "X = tokenizer.texts_to_sequences(df['cleaned'])\n",
    "\n",
    "# Pad all sequences to a fixed length of 50\n",
    "X = pad_sequences(X, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ 6. Encode Labels\n",
    "\n",
    "We convert our 'ham' and 'spam' labels into numerical format: `ham = 0` and `spam = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['label'])\n",
    "\n",
    "print(f\"Original labels: {df['label'].unique()}\")\n",
    "print(f\"Encoded labels: {encoder.transform(df['label'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† 7. Build and Train the Model\n",
    "\n",
    "We'll build our neural network and train it on the prepared training data. We use the test data as a validation set to monitor performance on unseen data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=16, input_length=MAX_LEN),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.1), # Add dropout for regularization\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà 8. Evaluate the Model\n",
    "\n",
    "After training, we perform a final evaluation on our test set. We'll look at:\n",
    "- **Loss & Accuracy**: Basic performance metrics.\n",
    "- **Classification Report**: Detailed metrics like precision, recall, and F1-score for each class.\n",
    "- **Confusion Matrix**: A table showing how many predictions were correct and incorrect for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_prob > 0.5, 1, 0)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ 9. Spam/Ham Prediction Demo\n",
    "\n",
    "Let's test our trained model on some new, unseen messages. This function simulates the entire preprocessing pipeline and feeds the message to the model for a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message(msg):\n",
    "    # Preprocess the message using the same steps as training\n",
    "    cleaned_msg = clean_text(msg)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned_msg])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    \n",
    "    # Get the prediction\n",
    "    pred_prob = model.predict(padded)[0][0]\n",
    "    prediction = \"Spam üö´\" if pred_prob > 0.5 else \"Ham ‚úÖ\"\n",
    "    \n",
    "    return f\"Prediction: {prediction} (Confidence: {pred_prob:.2f})\"\n",
    "\n",
    "# Test with a spam message\n",
    "spam_message = \"Congratulations! you have won a free lottery ticket worth $1000. call 12345 to claim now\"\n",
    "print(f\"Message: '{spam_message}'\")\n",
    "print(predict_message(spam_message))\n",
    "\n",
    "# Test with a ham message\n",
    "ham_message = \"Hi, can we meet tomorrow for the project discussion?\"\n",
    "print(f\"\\nMessage: '{ham_message}'\")\n",
    "print(predict_message(ham_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê 10. Export for TensorFlow.js\n",
    "\n",
    "Finally, we'll save our trained model and the tokenizer's word index. These files are all you need to run the spam classifier on your website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "import json\n",
    "\n",
    "# Save the model in TensorFlow.js format\n",
    "tfjs.converters.save_keras_model(model, 'model')\n",
    "\n",
    "# Save the tokenizer's word index\n",
    "word_index = tokenizer.word_index\n",
    "with open('word_index.json', 'w') as f:\n",
    "    json.dump(word_index, f)\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer saved for web deployment.\")\n",
    "print(\"\\nDon't forget to download the 'model' directory and the 'word_index.json' file!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


