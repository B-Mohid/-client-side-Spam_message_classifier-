{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Spam / Ham Message Classifier (Enhanced)\n",
    "\n",
    "Welcome to your custom spam classifier notebook! The goal of this notebook is to build, train, and export a machine learning model that can distinguish between 'spam' (unwanted messages) and 'ham' (legitimate messages).\n",
    "\n",
    "This enhanced version includes:\n",
    "- **Advanced Preprocessing**: Stopword removal and stemming.\n",
    "- **Data Analysis**: Justification for padding length.\n",
    "- **Robust Evaluation**: A full classification report and confusion matrix.\n",
    "- **Live Demo**: A cell to test predictions on new messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 1. Install Dependencies\n",
    "\n",
    "First, we need to install the necessary Python libraries. \n",
    "- `tensorflow`: The core machine learning library for building and training our model.\n",
    "- `tensorflowjs`: A library to convert our trained TensorFlow model into a format that can run directly in a web browser.\n",
    "- `nltk`: A library for natural language processing, used here for stopword removal and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflowjs nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📁 2. Load and Prepare the Dataset\n",
    "\n",
    "Next, we'll load our dataset. We're using the 'SMSSpamCollection' dataset, which is a collection of SMS messages already labeled as either spam or ham.\n",
    "\n",
    "**Action Required:** You'll need to upload the `SMSSpamCollection` file to your Colab environment. You can do this by clicking the **folder icon** on the left sidebar and then clicking the **upload button**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The dataset is a tab-separated file (.tsv), so we use sep='\\t'.\n",
    "# We also assign column names 'label' and 'message' for clarity.\n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "# Display the first 5 rows to verify it loaded correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧹 3. Enhanced Text Preprocessing\n",
    "\n",
    "To improve accuracy, we'll perform more advanced text cleaning:\n",
    "- **Tokenization**: Splitting sentences into words.\n",
    "- **Stopword Removal**: Removing common words (like 'the', 'a', 'is') that don't add much meaning.\n",
    "- **Stemming**: Reducing words to their root form (e.g., 'running' becomes 'run')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text) # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with a single space\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Tokenization, Stopword Removal, and Stemming\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "df['cleaned'] = df['message'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 4. Analyze Message Length Distribution\n",
    "\n",
    "To choose an optimal padding length (`maxlen`), it's helpful to understand the distribution of message lengths in our dataset. A fixed input length is required for our neural network, so we'll pad or truncate messages to fit. This plot helps us pick a value that covers most message lengths without adding excessive padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "message_lengths = df['cleaned'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(message_lengths, bins=50, kde=True)\n",
    "plt.title('Distribution of Message Lengths')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"From the chart, we can see that the vast majority of messages have fewer than 50 words. Choosing maxlen=50 is a reasonable choice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔢 5. Tokenize and Pad Sequences\n",
    "\n",
    "Now we convert our preprocessed text into numerical sequences and ensure they all have the same length (50), as determined from our analysis above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Use the top 5000 most frequent words for our vocabulary\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df['cleaned'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "X = tokenizer.texts_to_sequences(df['cleaned'])\n",
    "\n",
    "# Pad all sequences to a fixed length of 50\n",
    "X = pad_sequences(X, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 6. Encode Labels\n",
    "\n",
    "We convert our 'ham' and 'spam' labels into numerical format: `ham = 0` and `spam = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['label'])\n",
    "\n",
    "print(f\"Original labels: {df['label'].unique()}\")\n",
    "print(f\"Encoded labels: {encoder.transform(df['label'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 7. Build and Train the Model\n",
    "\n",
    "We'll build our neural network and train it on the prepared training data. We use the test data as a validation set to monitor performance on unseen data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=16, input_length=MAX_LEN),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.1), # Add dropout for regularization\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📈 8. Evaluate the Model\n",
    "\n",
    "After training, we perform a final evaluation on our test set. We'll look at:\n",
    "- **Loss & Accuracy**: Basic performance metrics.\n",
    "- **Classification Report**: Detailed metrics like precision, recall, and F1-score for each class.\n",
    "- **Confusion Matrix**: A table showing how many predictions were correct and incorrect for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_prob > 0.5, 1, 0)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 9. Spam/Ham Prediction Demo\n",
    "\n",
    "Let's test our trained model on some new, unseen messages. This function simulates the entire preprocessing pipeline and feeds the message to the model for a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message(msg):\n",
    "    # Preprocess the message using the same steps as training\n",
    "    cleaned_msg = clean_text(msg)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned_msg])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    \n",
    "    # Get the prediction\n",
    "    pred_prob = model.predict(padded)[0][0]\n",
    "    prediction = \"Spam 🚫\" if pred_prob > 0.5 else \"Ham ✅\"\n",
    "    \n",
    "    return f\"Prediction: {prediction} (Confidence: {pred_prob:.2f})\"\n",
    "\n",
    "# Test with a spam message\n",
    "spam_message = \"Congratulations! you have won a free lottery ticket worth $1000. call 12345 to claim now\"\n",
    "print(f\"Message: '{spam_message}'\")\n",
    "print(predict_message(spam_message))\n",
    "\n",
    "# Test with a ham message\n",
    "ham_message = \"Hi, can we meet tomorrow for the project discussion?\"\n",
    "print(f\"\\nMessage: '{ham_message}'\")\n",
    "print(predict_message(ham_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌐 10. Export for TensorFlow.js\n",
    "\n",
    "Finally, we'll save our trained model and the tokenizer's word index. These files are all you need to run the spam classifier on your website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "import json\n",
    "\n",
    "# Save the model in TensorFlow.js format\n",
    "tfjs.converters.save_keras_model(model, 'model')\n",
    "\n",
    "# Save the tokenizer's word index\n",
    "word_index = tokenizer.word_index\n",
    "with open('word_index.json', 'w') as f:\n",
    "    json.dump(word_index, f)\n",
    "\n",
    "print(\"✅ Model and tokenizer saved for web deployment.\")\n",
    "print(\"\\nDon't forget to download the 'model' directory and the 'word_index.json' file!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


